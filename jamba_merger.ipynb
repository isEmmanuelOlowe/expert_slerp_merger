{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"ai21labs/Jamba-v0.1\"\n",
    "\n",
    "temp_dir = \"/home/emmanuel/Documents/moe_merger/tmp/temp\" # replace with your own temp directory, user absolute path\n",
    "model_name = model_name_or_path.split(\"/\")[-1]\n",
    "tmp_dir = f\"{temp_dir}/{model_name}\"\n",
    "save_dir   =  \"/home/emmanuel/Documents/moe-bamba/tmp/Jamba-4xMoE_slerp\" # replace with your own save directory, use absolute path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {temp_dir}\n",
    "%cd {temp_dir}\n",
    "import os\n",
    "save_model_dir = model_name.split('/')[-1]\n",
    "!mkdir -p {save_model_dir}\n",
    "\n",
    "!wget https://huggingface.co/{model_name_or_path}/resolve/main/config.json -O {save_model_dir}/config.json\n",
    "!wget https://huggingface.co/{model_name_or_path}/resolve/main/model.safetensors.index.json -O {save_model_dir}/model.safetensors.index.json\n",
    "!wget https://huggingface.co/{model_name_or_path}/resolve/main/generation_config.json -O {save_model_dir}/generation_config.json\n",
    "\n",
    "for i in range(1,22):\n",
    "    file_count_str = str(i).zfill(5)\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(f\"{save_model_dir}/model-{file_count_str}-of-00021.safetensors\"):\n",
    "        !wget https://huggingface.co/{model_name_or_path}/resolve/main/model-{file_count_str}-of-00021.safetensors?download=true -O {save_model_dir}/model-{file_count_str}-of-00021.safetensors\n",
    "    else:\n",
    "        print(f\"model-{file_count_str}-of-00021.safetensors already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {temp_dir}\n",
    "\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "from safetensors import safe_open\n",
    "from safetensors.torch import save_file\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def tensor_load(file_name, map_location=None):\n",
    "    tensors = {}\n",
    "    with safe_open(file_name, framework=\"pt\") as f:\n",
    "        for k in f.keys():\n",
    "            tensors[k] = f.get_tensor(k)\n",
    "    return tensors\n",
    "\n",
    "def get_weight_byte_size(weight):\n",
    "\n",
    "    if isinstance(weight, torch.Tensor):\n",
    "        weight_byte_size = weight.nelement() * weight.element_size()\n",
    "    else:\n",
    "        weight_byte_size = sum(p.nelement() * p.element_size() for p in weight.parameters())\n",
    "\n",
    "    return weight_byte_size\n",
    "\n",
    "def merge_tensor(tensorA, tensorB):\n",
    "\n",
    "    t = 0.5\n",
    "\n",
    "    dot = torch.sum(tensorA * tensorB, dim=1)\n",
    "    norm_v0 = torch.norm(tensorA, dim=1)\n",
    "    norm_v1 = torch.norm(tensorB, dim=1)\n",
    "    cos_omega = dot / (norm_v0 * norm_v1)\n",
    "\n",
    "    eps = 1e-6\n",
    "    cos_omega = torch.clamp(cos_omega, -1 + eps, 1 - eps)\n",
    "    omega = torch.acos(cos_omega)\n",
    "\n",
    "    # Slerp\n",
    "    v_t = (torch.sin((1 - t) * omega) / torch.sin(omega)).unsqueeze(1) * tensorA \\\n",
    "          + (torch.sin(t * omega) / torch.sin(omega)).unsqueeze(1) * tensorB\n",
    "\n",
    "    return v_t\n",
    "\n",
    "def compute_total_size(weight_map):\n",
    "    total_size = 0\n",
    "    for key in weight_map.keys():\n",
    "        weight = tensor_load(f\"{save_dir}/{weight_map[key]}\", map_location=\"cpu\")\n",
    "        total_size += get_weight_byte_size(weight[key])\n",
    "    return total_size\n",
    "\n",
    "def merge_model(model_name_or_path, tmp_dir, save_dir, num_experts_per_tok=2, num_local_experts=2):\n",
    "\n",
    "    def compute_total_size(weight_map):\n",
    "        total_size = 0\n",
    "        for key in weight_map.keys():\n",
    "            weight = tensor_load(f\"{save_dir}/{weight_map[key]}\", map_location=\"cpu\")\n",
    "            total_size += get_weight_byte_size(weight[key])\n",
    "        return total_size\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    config_path = f\"{save_dir}/config.json\"\n",
    "    # currently only support for Jamba model\n",
    "    current_num_experts_per_tok = 2\n",
    "    current_num_local_experts = 16\n",
    "    config = None\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "        config[\"num_experts_per_tok\"] = num_experts_per_tok\n",
    "        config[\"num_experts\"] = num_local_experts\n",
    "\n",
    "    divisor = current_num_local_experts // num_local_experts\n",
    "    print(\"Divisor:\", divisor)\n",
    "    # save config\n",
    "    with open(f\"{save_dir}/config.json\", \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "\n",
    "    # weight\n",
    "    weight_map = {}\n",
    "    first_weights = [\"lm_head.weight\", \"model.embed_tokens.weight\", \"model.final_layernorm.weight\"]\n",
    "\n",
    "    # load weight map\n",
    "    bin_index_path = f\"{tmp_dir}/model.safetensors.index.json\"\n",
    "    with open(bin_index_path, \"r\") as f:\n",
    "        weight_map = json.load(f)[\"weight_map\"]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # load weight map\n",
    "    layers = {}\n",
    "    for key in weight_map.keys():\n",
    "        if key in first_weights:\n",
    "            continue\n",
    "        \n",
    "        # print(\"key\", key)\n",
    "        # keyが\"model.layers.[0-9]+.\"にmatchする場合はlayers_listに追加する\n",
    "        layer_str = re.match(r\"model\\.layers\\.[0-9]+\\.\", key)[0]\n",
    "        if layer_str:\n",
    "            layer_no = re.findall(r\"\\d+\",layer_str)\n",
    "            layer_no = layer_no[0]\n",
    "            if layer_no not in layers.keys():\n",
    "                layers[layer_no] = []\n",
    "\n",
    "            layers[layer_no].append({ \"key\":key, \"file_name\":weight_map[key] })\n",
    "\n",
    "    # new weight_map index\n",
    "    new_weight_map = {\n",
    "    \"metadata\": {\n",
    "        \"total_size\": 0\n",
    "    },\n",
    "    \"weight_map\": {\n",
    "    }\n",
    "    }\n",
    "\n",
    "    # load tensors\n",
    "    tensor_weights = {}\n",
    "    tensors = {}\n",
    "    current_file_name = \"\"\n",
    "\n",
    "    file_count = 0\n",
    "    file_count_str = str(file_count).zfill(5)\n",
    "\n",
    "    for key in first_weights:\n",
    "        file_name = weight_map[key]\n",
    "        if current_file_name != file_name:\n",
    "\n",
    "            # load safetensor\n",
    "            tensors = tensor_load(f\"{tmp_dir}/{file_name}\", map_location=\"cpu\")\n",
    "            current_file_name = file_name\n",
    "\n",
    "        tensor_weights[key] = tensors[key]\n",
    "        new_weight_map[\"weight_map\"][key] = f\"model-{file_count_str}.safetensors\"\n",
    "\n",
    "    # save tensor\n",
    "    save_file(tensor_weights, f\"{save_dir}/model-{file_count_str}.safetensors\", metadata={\"format\":\"pt\"})\n",
    "    file_count += 1\n",
    "\n",
    "    layer_keys = sorted([ int(k) for k in layers.keys()])\n",
    "    print(\"num_layer_keys\", len(layer_keys))\n",
    "    for layer_no in layer_keys:\n",
    "        print(\"starting layer:\",layer_no)\n",
    "        file_count_str = str(file_count).zfill(5)\n",
    "        tensor_weights = {}\n",
    "\n",
    "        stock_expert_weights = {}\n",
    "\n",
    "        current_file_name = \"\"\n",
    "        for info in layers[str(layer_no)]:\n",
    "            file_name = info[\"file_name\"]\n",
    "            if current_file_name != file_name:\n",
    "                print(\"Loading Tensors \", file_name)\n",
    "                tensors = tensor_load(f\"{tmp_dir}/{file_name}\", map_location=\"cpu\")\n",
    "                current_file_name = file_name\n",
    "\n",
    "            layer_key = info[\"key\"]\n",
    "            layer_weights = tensors[layer_key]\n",
    "\n",
    "            if '.moe.experts' in layer_key:\n",
    "                \n",
    "\n",
    "                lk = re.findall(r\"[.]experts[.][0-9]+.\", layer_key)[0]\n",
    "                exp_index = int( re.findall(r\"\\d+\",lk)[0] )\n",
    "\n",
    "                new_layer_key = re.sub(r\"\\.experts\\.\\d+\\.\", f\".experts.{exp_index // divisor}.\", layer_key)\n",
    "\n",
    "                if new_layer_key not in stock_expert_weights.keys():\n",
    "                    tensor_weights[new_layer_key] = layer_weights\n",
    "                    new_weight_map[\"weight_map\"][new_layer_key] = f\"model-{file_count_str}.safetensors\"\n",
    "                else:\n",
    "                    # merge experts\n",
    "                    tensor_weights[new_layer_key] = merge_tensor(tensor_weights[layer_key] , layer_weights)\n",
    "\n",
    "                print(\"merging expert\", exp_index, \"to\", new_layer_key, tensor_weights[new_layer_key].shape)\n",
    "\n",
    "                if exp_index % divisor == divisor - 1:\n",
    "                    print(\"new experts\", new_layer_key, tensor_weights[new_layer_key].shape, \"from\", layer_key)\n",
    "\n",
    "\n",
    "            elif '.moe.router' in layer_key:\n",
    "                # print(\"reshape\", layer_weights.shape, \"-> view(2, 4, 4096) -> (2, 4096)\", layer_key)\n",
    "                print(\"reshape\", layer_weights.shape, f\"-> view({num_local_experts}, {divisor}, 4096) -> ({num_local_experts}, 4096)\", layer_key)\n",
    "\n",
    "                # calc gate merge\n",
    "                weights_reshaped = layer_weights.view(num_local_experts, divisor, 4096)\n",
    "\n",
    "                for i in range(divisor):\n",
    "                    if i == 0:\n",
    "                        tensor_weights[layer_key] = weights_reshaped[:, i, :]\n",
    "                        new_weight_map[\"weight_map\"][layer_key] = f\"model-{file_count_str}.safetensors\"\n",
    "                    else:\n",
    "                        tensor_weights[layer_key] = merge_tensor(tensor_weights[layer_key], weights_reshaped[:, i, :])\n",
    "\n",
    "\n",
    "            else:\n",
    "                tensor_weights[layer_key] = layer_weights\n",
    "\n",
    "                new_weight_map[\"weight_map\"][layer_key] = f\"model-{file_count_str}.safetensors\"\n",
    "\n",
    "        # save tensor\n",
    "        save_file(tensor_weights, f\"{save_dir}/model-{file_count_str}.safetensors\", metadata={\"format\":\"pt\"})\n",
    "        print(\"Save Tensors \", f\"{save_dir}/model-{file_count_str}.safetensors\")\n",
    "        file_count += 1\n",
    "\n",
    "    # save new_weight_map\n",
    "    new_weight_map[\"metadata\"][\"total_size\"] = compute_total_size(new_weight_map[\"weight_map\"])\n",
    "    with open(f\"{save_dir}/model.safetensors.index.json\", \"w\") as f:\n",
    "        json.dump(new_weight_map, f, indent=2)\n",
    "\n",
    "merge_model(model_name_or_path, tmp_dir, save_dir, num_experts_per_tok=2, num_local_experts=4) # Makes the 4xMoE model\n",
    "print(\"Merge completed. Model saved at:\", save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
